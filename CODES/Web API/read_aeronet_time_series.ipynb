{"cells":[{"cell_type":"markdown","metadata":{"id":"_JoDqu-wN15W"},"source":["- **Module:** read_aeronet_time_series.ipynb\n","- **Authors:** Petar Grigorov and Pawan Gupta\n","- **Organization:** NASA AERONET (https://aeronet.gsfc.nasa.gov/)\n","- **Date:** 07/03/2023\n","- **Last Revision:** 07/29/2024\n","- **Purpose:** Time-series analysis of AERONET sites AOD levels\n","- **Disclaimer:** The code is for demonstration purposes only. Users are responsible to check for accuracy and revise to fit their objective.\n","- **Contact:** Report any concern or question related to the code to pawan.gupta@nasa.gov or petar.t.grigorov@nasa.gov\n","- **Readme:** https://github.com/pawanpgupta/AERONET/blob/Python/README/Read_AERONET_TimeSeries"]},{"cell_type":"markdown","metadata":{"id":"CzzFuiyGzBEr"},"source":["**Required packages installation and importing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdEwG0LLR4KW"},"outputs":[],"source":["!pip uninstall -y numpy pandas\n","!pip install numpy==1.26.4 pandas==2.0.3\n","!pip install beautifulsoup4\n","!pip install requests\n","!pip install calplot\n","\n","from bs4 import BeautifulSoup      #reads data from website (web scraping)\n","import re                          #regular expression matching operations (RegEx)\n","import requests                    #useful for sending HTTP requests\n","import shutil                      #useful for creating zip files\n","import math                        #useful math operations\n","import numpy as np                 #for array manipulation\n","from numpy import arange           #range of values for testing purposes\n","import datetime                    #for time data manipulation\n","import pandas as pd                #for data querying and processing\n","import matplotlib.pyplot as plt    #for creating plots\n","import matplotlib.dates as mdates  #for converting datetime to numeric\n","import calplot                     #for creating heat maps\n","from collections import Counter    #for keeping track of unique data\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"8ulHy6DWzVFS"},"source":["**Connecting and mounting local drive onto colab notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fe9Drz39Sqrf"},"outputs":[],"source":["from google.colab import files      #ensures output zip file can be downloaded\n","from google.colab import drive      #imports local google drive\n","drive.mount('/drive')               #mounts local google drive onto colab\n","!mkdir Output_TimeSeries            #makes directory where output files will be stored"]},{"cell_type":"markdown","metadata":{"id":"NWd23a3vN7v9"},"source":["**Setup input parameters such as date, data level, averaging type, AOD range for mapping, AOD/Angstrom exponent, and geographical limits**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDPyzlBzN6kF"},"outputs":[],"source":["site = 'GSFC'                           #Please make sure site name is spelled properly\n","dt_initial = '20140102'                 #starting date YYYYMMDD format\n","dt_final = '20240724'                   #final date YYYYMMDD format\n","level = 1.5                             #AERONET data level\n","average_type = 1                        #daily (1), monthly (2)\n","feature_choice = 1                      #Enter '1' if you are specifying an AOD wavelength or '2' if you are specifying an Angstrom exponent\n","wavelength = 500                        #Available choices: 1640, 1020, 870, 865, 779, 675, 667, 620, 560, 555, 551, 532, 531, 510, 500, 490, 443, 440, 412, 400, 380, 340\n","Angstrom_exp = '440-675'                #Available choices: '440-870','380-500','440-675','500-870','340-440','440-675(Polar)'"]},{"cell_type":"markdown","metadata":{"id":"5CSmQFb4zpzV"},"source":["**Get desired AERONET data using web services, then scraping data from website**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8AgFuQ8Sxy_"},"outputs":[],"source":["yr_initial = dt_initial[:4]               #initial year\n","mon_initial = dt_initial[4:6]             #initial month\n","day_initial = dt_initial[6:]              #initial day\n","\n","yr_final = dt_final[:4]                   #final year\n","mon_final = dt_final[4:6]                 #final month\n","day_final = dt_final[6:]                  #final day\n","\n","if level == 1 or level == 1.0:\n","  level = 10\n","elif level == 1.5:\n","  level = 15\n","elif level == 2 or level == 2.0:\n","  level = 20\n","else:\n","  print(\"\\nIncorrect input for data level type. Defaulting to level 1.5...\")\n","  level = 15\n","\n","if level == 20 and int(yr_initial) == datetime.date.today().year:                 #if user wants level 2 data for the current year, program alerts that data may not be available\n","  level = 15                                                                      #defaults to level 1.5 data\n","  print(\"\\nThere is no level 2 data available for the current year. Defaulting to level 1.5 data...\")\n","\n","url = 'https://aeronet.gsfc.nasa.gov/cgi-bin/print_web_data_v3?site='+site+'&year='+yr_initial+'&month='+mon_initial+'&day='+day_initial+'&year2='+yr_final+'&month2='+mon_final+'&day2='+day_final+'&AOD'+str(level)+'=1&AVG=20'\n","soup = BeautifulSoup(requests.get(url).text) #web services contents are read here from URL"]},{"cell_type":"markdown","metadata":{"id":"6lr2tP3DSbU3"},"source":["**Writes soup data to text file, assigns contents to Pandas dataframe, prepares data for plotting**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXOqVzEABbWu"},"outputs":[],"source":["with open(r'/content/temp.txt' ,\"w\") as oFile:          #writes the data scraped from \"beautiful soup\" to a text file on your local Google drive\n","    oFile.write(str(soup.text))\n","    oFile.close()\n","\n","df = pd.read_csv(r'/content/temp.txt',skiprows = 5)     #loads the csv data into a Pandas dataframe\n","!rm temp.txt\n","\n","if len(df) > 0:\n","  df = df.replace(-999.0, np.nan)                                     #replaces all -999.0 vakyes with NaN; helps with accurate data aggregation\n","  df[['Day','Month','Year']] = df['Date(dd:mm:yyyy)'].str.split(':',expand=True)                                #splits the date column and then joins it back together using \"-\" instead of \":\"\n","  df['Date'] = df[['Year','Month','Day']].apply(lambda x: '-'.join(x.values.astype(str)), axis=\"columns\")       #because datetime format in python does not recognize colons\n","  df['Date']= pd.to_datetime(df['Date'])                                              #converts the new date column to datetime format\n","else:\n","  print(\"No data to parse. Please retry with different parameters.\")"]},{"cell_type":"markdown","metadata":{"id":"AQPcXhn81AEp"},"source":["**AOD Wavelength or Angstrom Exponent Selection**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceE04Eg9S8ca"},"outputs":[],"source":["AOD_col = [col for col in df.columns if 'AOD_' in col and 'nm' in col] #list of AOD columns, used for mapping user input to them\n","AOD_col = [item for item in AOD_col if 'N[' not in item]\n","Ang_exp_col = [col for col in df.columns if 'Angstrom_Exponent' in col] #list of Angstrom Exponent columns, used for mapping user input to them\n","Ang_exp_col = [item for item in Ang_exp_col if 'N[' not in item]\n","\n","AOD_val = [int(re.search(r'\\d+', col).group()) for col in AOD_col] #expected user input choices for AOD\n","Ang_exp_val = [item.split('_')[0] for item in Ang_exp_col] #expected user input choices for AE\n","Ang_exp_val[-1] += '(Polar)' #manually adds the polar channel to the list\n","\n","if feature_choice == 1:\n","  if wavelength in AOD_val:             #if user input for AOD wavelength matches a value in the list, code proceeds forward. Otherwise it prompts user to try again\n","    for i in range(len(AOD_col)):\n","      if wavelength == AOD_val[i]:      #code scans the list of columns and list of possible values, and matches user input to the appropriate column name\n","        df = df[['Date','Day_of_Year',AOD_col[i]]]         #if a match exists, the column name is matched to the actual column and it is then appended to the dataset\n","  else:\n","    df = df[['Date','Day_of_Year','AOD_500nm']]\n","    print(\"\\nInput for AOD wavelength is not in list. Defaulting to 500nm...\")\n","elif feature_choice == 2:\n","  if Angstrom_exp in Ang_exp_val:     #if user input for Angstrom Exponent matches a value in the list, code proceeds forward. Otherwise it prompts user to try again\n","    for i in range(len(Ang_exp_col)):\n","      if Angstrom_exp == Ang_exp_val[i]:  #code scans the list of columns and list of possible values, and matches user input to the appropriate column nam\n","        df = df[['Date','Day_of_Year',Ang_exp_col[i]]]     #if a match exists, the column name is matched to the actual column and it is then appended to the dataset\n","  else:\n","    df = df[['Date','Day_of_Year','440-675']]\n","    print(\"\\nInput for Angstrom Exponent is not in list. Defaulting to 440-675...\")\n","else:\n","  feature_choice == 1\n","  print(\"\\nIncorrect input for feature choice. Defaulting to AOD wavelength (1)...\")\n","  if wavelength in AOD_val:             #if user input for AOD wavelength matches a value in the list, code proceeds forward. Otherwise it prompts user to try again\n","    for i in range(len(AOD_col)):\n","      if wavelength == AOD_val[i]:      #code scans the list of columns and list of possible values, and matches user input to the appropriate column name\n","        df = df[['Date','Day_of_Year',AOD_col[i]]]         #if a match exists, the column name is matched to the actual column and it is then appended to the dataset\n","  else:\n","    df = df[['Date','Day_of_Year','AOD_500nm']]\n","    print(\"\\nInput for AOD wavelength is not in list. Defaulting to 500nm...\")\n","\n","df = df.dropna().reset_index(drop=True) #Drops NaN or -999.0 values\n","df"]},{"cell_type":"markdown","metadata":{"id":"xpOQg_IJ9Mtz"},"source":["**Time-series analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SqOeWmf9MTj"},"outputs":[],"source":["if average_type == 2:\n","  df = df.groupby(pd.PeriodIndex(df['Date'], freq=\"M\")).mean().reset_index()\n","\n","plot = df.plot('Date', df.columns[-1], title = \"Site: \"+str(site), figsize=(16,8))\n","plt.scatter(df['Date'], df[df.columns[-1]])    #superimposed scatter plot of points, in addition to the line graph\n","plt.ylabel(df.columns[-1].replace('_', ' '))\n","plot.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # Set the x-axis format to display dates\n","fig = plot.get_figure()\n","fig.savefig('/content/Output_TimeSeries/TimeSeries_'+str(site)+'.png')               #saves figure\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tRj-CkqG-pRC"},"source":["**Tile Map of Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7ABvWi37nIq"},"outputs":[],"source":["if average_type == 1:\n","  df[['Year','Month','Day']] = df['Date'].astype(str).str.split('-',expand=True)\n","  df['Month'] = df['Month'].astype(int)\n","  num_missing_months = 12 - len(set(df['Month'].to_list()))\n","  df = df.drop(columns = ['Month','Day'])\n","  df['Year'] = df['Year'].astype(int)\n","\n","  # Extracting year, month, and values from the data\n","  years = df['Year'].to_list()\n","  num_missing_years = max(years) - min(years) + 1 - len(set(years))\n","  days = df['Day_of_Year'].to_list()\n","  months = np.arange(1, 13, 1).tolist()\n","  values = df.iloc[:,2].to_list()\n","\n","  # Creating a 2D matrix to represent the heatmap\n","  heatmap = np.zeros((max(years) - min(years) + 1, max(days)))\n","\n","  # Populating the heatmap with values\n","  for i in range(len(df)):\n","      year_index = years[i] - min(years)\n","      day_index = days[i] - 1\n","      heatmap[year_index, day_index] = values[i]\n","\n","  #assign all zeroes (missing values) in heatmap matrix as NaN so that matplotlib set_bad function can work\n","  num_rows, num_cols = heatmap.shape\n","  for i in range(num_rows):\n","    for j in range(num_cols):\n","      if heatmap[i,j] == 0.0:\n","        heatmap[i,j] = 'NaN'\n","\n","  # Plotting the heatmap\n","  plt.rcParams[\"figure.figsize\"] = [18, 8]\n","  ax= plt.subplot()\n","  current_cmap = plt.cm.get_cmap('RdYlGn_r')\n","  current_cmap.set_bad(color = 'white')\n","  current_cmap.set_extremes(under='gray', over='magenta')\n","  plt.imshow(heatmap, cmap = current_cmap, interpolation='nearest', aspect='auto', vmin = 0, vmax = 1.0)\n","  plt.colorbar(label=df.columns[2], extend = 'both')\n","  month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec']\n","  plt.xticks(np.arange(0, 365, 31), month_labels)\n","  plt.yticks(np.arange(len(Counter(years).keys())+num_missing_years), np.arange(min(years), max(years) + 1))\n","  plt.title('Site: '+str(site), size=12)\n","  plt.savefig('/content/Output_TimeSeries/TilePlot_Daily_'+str(site)+'.png')\n","  plt.show()\n","\n","elif average_type == 2:\n","  df[['Year','Month']] = df['Date'].astype(str).str.split('-',expand=True)\n","  df['Year'] = df['Year'].astype(int)\n","  df['Month'] = df['Month'].astype(int)\n","\n","  # Extracting year, month, and values from the data\n","  years = df['Year'].to_list()\n","  num_missing_years = max(years) - min(years) + 1 - len(set(years))\n","  months = df['Month'].to_list()\n","  values = df.iloc[:,2].to_list()\n","\n","  # Creating a 2D matrix to represent the heatmap\n","  heatmap = np.zeros((max(years) - min(years) + 1, max(months)))\n","\n","  # Populating the heatmap with values\n","  for i in range(len(df)):\n","      year_index = years[i] - min(years)\n","      month_index = months[i] - 1\n","      heatmap[year_index, month_index] = values[i]\n","\n","  #assign all zeroes (missing values) in heatmap matrix as NaN so that matplotlib set_bad function can work\n","  num_rows, num_cols = heatmap.shape\n","  for i in range(num_rows):\n","    for j in range(num_cols):\n","      if heatmap[i,j] == 0.0:\n","        heatmap[i,j] = 'NaN'\n","\n","  # Plotting the heatmap\n","  plt.rcParams[\"figure.figsize\"] = [12, 8]\n","  ax= plt.subplot()\n","  current_cmap = plt.cm.get_cmap('RdYlGn_r')\n","  current_cmap.set_bad(color = 'white')\n","  current_cmap.set_extremes(under='gray', over='magenta')\n","  plt.imshow(heatmap, cmap = current_cmap, interpolation='nearest', aspect='auto')\n","  plt.colorbar(label=df.columns[2], extend = 'both')\n","  plt.xticks(np.arange(len(Counter(months).keys())), np.arange(1, len(Counter(months).keys()) + 1))\n","  plt.yticks(np.arange(len(Counter(years).keys())+num_missing_years), np.arange(min(years), max(years) + 1))\n","  ax.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sept','Oct','Nov','Dec'], fontsize=10)\n","  plt.title('Site: '+str(site), size=12)\n","  plt.savefig('/content/Output_TimeSeries/TilePlot_Monthly_'+str(site)+'.png')\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4IrPARdkzsZJ"},"source":["**Calendar plot of Time series (Daily averages only)**"]},{"cell_type":"code","source":["if average_type == 1:\n","\n","  df_cal = df.set_index('Date')\n","  df_cal.index = pd.DatetimeIndex(df_cal.index)\n","  df_cal = df_cal.reindex(pd.date_range(df['Date'].min(), df['Date'].max()), fill_value=0)\n","\n","  current_cmap = plt.cm.get_cmap('RdYlGn_r')\n","  current_cmap.set_extremes(under='white', over='magenta')\n","\n","  fig, ax = calplot.calplot(df_cal[df_cal.columns[1]], edgecolor='black', linewidth=2, colorbar=False,\n","                          suptitle=\"Site: \"+str(site), suptitle_kws={'size': 11, 'weight':'bold'},\n","                          cmap = current_cmap, vmin=df[df.columns[2]].min(), vmax=1)\n","\n","  fig.colorbar(ax[0].get_children()[1], ax = ax.ravel().tolist(), extend='both',\n","               orientation = 'horizontal', aspect = 50, pad = 0.03, label = df_cal.columns[1])\n","\n","  plt.savefig('/content/Output_TimeSeries/CalendarPlot_Daily_'+str(site)+'.png', bbox_inches='tight')"],"metadata":{"id":"l0XurvQCIgaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Annual variability plot (Daily averages only)**"],"metadata":{"id":"tFqtIX71Qoiz"}},{"cell_type":"code","source":["if average_type == 1:\n","  df_annual = df[['Year',df.columns[2]]]   #isolates year and AOD columns to new dataframe\n","  df_annual = pd.merge(df_annual,df_annual.groupby(['Year']).size().reset_index().rename(columns = {0:\"Count\"})) #counts total number of yearly data\n","  df_annual = df_annual[df_annual.Count >= 30] #filters out years with less than 30 measurements\n","  df_annual = df_annual.drop(columns=['Count']).reset_index(drop=True) #drops Count column after filtering dataset\n","\n","  if len(df_annual['Year'].unique()) > 1:\n","    df_sigma = df_annual.groupby(['Year']).std().reset_index()  #takes standard deviation of AOD measurements per year\n","    df_sigma = df_sigma.rename(columns={df.columns[2]:df.columns[2]+str('_sigma')})  #assigns standard deviation column name\n","    df_miu = df_annual.groupby(['Year']).mean().reset_index() #takes average of AOD measurements per year\n","    df_miu = df_miu.rename(columns={df.columns[2]:df.columns[2]+str('_miu')})  #assigns mean AOD column name\n","\n","    df_statistics = pd.merge(df_sigma, df_miu)  #merges yearly average and standard deviation of AOD measurements\n","    df_statistics['Upper'] = df_statistics[df_statistics.columns[2]] + df_statistics[df_statistics.columns[1]] #Upper bound of shaded region\n","    df_statistics['Lower'] = df_statistics[df_statistics.columns[2]] - df_statistics[df_statistics.columns[1]] #Lower bound of shaded region\n","\n","    plt.style.use('dark_background') #set dark background\n","    plt.plot(df_statistics['Year'], df_statistics[df_statistics.columns[2]],color='white',linewidth=3, label='Actual Data')  #creates line plot of Year vs average yearly measurement\n","    plt.fill_between(df_statistics['Year'], df_statistics[df_statistics.columns[2]]-df_statistics[df_statistics.columns[1]], df_statistics[df_statistics.columns[2]]+df_statistics[df_statistics.columns[1]], color='yellow', alpha = 0.8) #plot standard deviation\n","    plt.scatter(df_statistics['Year'], df_statistics[df_statistics.columns[2]],color='white',s=150) #superimposed scatter plot to show data as dots\n","    p = np.poly1d(np.polyfit(df_statistics['Year'], df_statistics[df_statistics.columns[2]], 1)) #calculates 1st order trendline\n","    plt.plot(df_statistics['Year'], p(df_statistics['Year']),color='red',linewidth=3,linestyle='dashed',label='Linear Regression') #plots trendline\n","    plt.xticks(np.arange(df_statistics['Year'].min(), df_statistics['Year'].max()+1, 1.0), fontsize=14, rotation=30, ha='center') #adjust xticks\n","    plt.yticks(np.arange(int(df_statistics['Lower'].min() * 10)/10.0, math.ceil(df_statistics['Upper'].max() * 10)/10.0, 0.025), fontsize=14) #adjust yticks\n","    plt.xlabel('Year', size=16, fontweight=\"bold\") #adjust xlabel size name and font\n","    plt.ylabel(df.columns[2].replace('_', ' '), size=16, fontweight=\"bold\") #adjust ylabel size name and font\n","    plt.title(\"Annual AOD Averages for \"+str(site)+\" Site with 1 Standard Deviation\", size=18, fontweight=\"bold\") #sets graph title\n","    #plt.text(x = df_annual['Year'].min(), y = df_statistics['Upper'].max(), s = 'Y(Linear) ='+str(p))\n","    #+'\\nR2 ='+str(r2_score(p(df_statistics['Year']), df_statistics[df_statistics.columns[2]]).round(3))+'\\n\\nY(Lasso) = '+str(p_lasso)+'\\nR2 ='+str(r2_score(p_lasso(Predicted_Lasso['Year']), df_statistics[df_statistics.columns[2]]).round(3)), ha='left', va='top',fontsize=16) #displays trendline and R2 value\n","    plt.legend() #displays legend\n","    plt.savefig('/content/Output_TimeSeries/AnnualAOD_Averages_'+str(site)+'.png') #saves figure as png\n","    plt.show() #display graph"],"metadata":{"id":"z8xot-FqQoFs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9MBKw1OqK2m"},"source":["**Download the saved time series plot as png file**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StFNOIU-MQga"},"outputs":[],"source":["while True:\n","  zip_download = str(input(\"Would you like to download your output in a zipped folder (y or n)?: \"))\n","  if zip_download == 'y' or zip_download == 'Y' or zip_download == 'Yes' or zip_download == 'yes':\n","    shutil.make_archive('Output_TimeSeries', 'zip', '/content/Output_TimeSeries')  #zips all output files\n","    files.download('Output_TimeSeries.zip')  #Note: Must use Chrome browser for download to work\n","    break\n","  elif zip_download == 'n' or zip_download == 'N' or zip_download == 'No' or zip_download == 'no':\n","    break\n","  else:\n","    print(\"\\nIncorrect input. Please try again!\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1137sHLtyfV8Y9n3M97nJEjMjn8Xi7oDC","timestamp":1688407803693},{"file_id":"1FIhxt_vHv_c-NM-zzqllTuyiTfmOoAWF","timestamp":1687836455523},{"file_id":"1iEED1qbaSd9K4kq4iPlIz8pKHB_vZ8xh","timestamp":1687181710931},{"file_id":"1kohDXtxZUANrGkkbl5YpPQ4i_UAB_qe9","timestamp":1687094628663}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}